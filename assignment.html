<h1 id="final-assignment">Final assignment</h1>
<p>Deadline: April 28th (sunday) at 23:00 (Paris time).</p>
<p>You must return your assignment as a single python notebook sent to me by email <code>guillaume.plique@sciencespo.fr</code>. The notebook should be an <code>.ipynb</code> file.</p>
<p>Don&#39;t forget to scrub your YouTube API key from the cells instantiating a <code>YouTubeAPIClient</code> to avoid giving me your key.</p>
<p>I would like to see how you collected YouTube data on the notebook, but don&#39;t hesitate to collect it beforehand and save it to JSON to avoid running a potentially time-consuming collection each time you need to run your analysis.</p>
<p>Don&#39;t worry if your notebook cannot be run on my end, I can very well assess your coding skills and analysis by only reading your code and comments.</p>
<p>Finally, don&#39;t forget that you can interleave markdown and python cells in your notebook, so make sure to explain what you are doing and what is the rationale of your analysis and conclusions within dedicated markdown cells (or python comments if you prefer).</p>
<h2 id="rationale-of-the-assignement">Rationale of the assignement</h2>
<p>Big picture: I want you to study the conversation on YouTube around a social issue that you deem interesting or relevant. The idea is to produce a python notebook that will be used to 1. collect data on YouTube using <code>minet</code> (as we learned to do together) and 2. to analyse the collected data by running python computations.</p>
<p>This means that you need to design a data collection protocol by finding where your social issue is talked about on YouTube, and it probably means to collect some amount of comments, video descriptions and/or captions.</p>
<p>Be sure to keep in mind that your queries might return too much data for your means (both time and disk space, basically), so think carefully ahead and try to draw an estimation of how long data collection will take you and what size it will take on disk. Between 1000 and 10,000 items is probably a good target.</p>
<p>Sampling is also a valid thing to do, as long as you explain how you chose to filter/limit the data collected.</p>
<p>Don&#39;t forget also that videos collected by channel have less metadata than what you can get with the <code>#.videos</code> method (refer to the relevant notebook for help).</p>
<h2 id="compulsory-analysis-elements">Compulsory analysis elements</h2>
<ol>
<li>Mean and median of both view and likes counts for all the collected videos</li>
<li>Top commenters by number of comments and likes</li>
<li>Counter of video topics</li>
</ol>
<h2 id="useful-documentation-links">Useful documentation links</h2>
<ul>
<li>Python <a href="https://docs.python.org/3/library/statistics.html"><code>statistics</code></a> module from the standard library</li>
<li>Documentation for the <a href="https://github.com/medialab/ural"><code>ural</code></a> library to deal with urls</li>
<li>Documentation for the <a href="https://github.com/medialab/minet/blob/master/docs/web.md"><code>minet.web</code></a> subpackage (useful if you want to resolve urls to draw better stats)</li>
</ul>
